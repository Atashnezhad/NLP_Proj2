{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function from: https://github.com/scaress21/reddit_and_quibi/blob/master/code/01A_Gathering_Reddit_Data.ipynb\n",
    "#Function to get posts from reddit\n",
    "def get_posts(subreddit, num, t):\n",
    "    #Setting the base url and first \"before\" time\n",
    "    base_url = 'https://api.pushshift.io/reddit/submission/search'\n",
    "    bef_time = t\n",
    "    \n",
    "    #list to hold the dataframes to concat\n",
    "    to_concat = []\n",
    "    \n",
    "    #While loop that keeps gathering until the number of desired posts is reached\n",
    "    while len(to_concat) < (num / 1000):\n",
    "        params = {\n",
    "            'subreddit' : subreddit,\n",
    "            'size' : 1000,\n",
    "            'before' : bef_time,\n",
    "            'lang' : True,\n",
    "            'author': '![deleted]'\n",
    "                }\n",
    "        get = requests.get(base_url, params)\n",
    "        data = get.json()['data']\n",
    "        df = pd.DataFrame(data)\n",
    "        bef_time = df['created_utc'].min()\n",
    "        to_concat.append(df)\n",
    "        \n",
    "        #If statement to print out updates every 5000 posts including the time of the earliest post\n",
    "        if len(to_concat) % 5 == 0:\n",
    "            #Converting the epoch time into a more readable, datetime format\n",
    "            print_time = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(bef_time))\n",
    "            print(f'{len(to_concat)*1000} posts have been gathered, oldest post is from {print_time}')\n",
    "    \n",
    "    #Once the while loop is done, concat the dataframes together and reset the index\n",
    "    master = pd.concat(to_concat, axis=0)\n",
    "    master.reset_index(inplace=True)\n",
    "    \n",
    "    #Making sure the posts are unique with unique ID's\n",
    "    duplicates = master['id'].duplicated().sum()\n",
    "    \n",
    "    #Final update confirming how many posts were gathered and if there are duplicates\n",
    "    print(f'Final DataFrame shape: {master.shape}, there are {duplicates} duplicates')\n",
    "    \n",
    "    #Return the final dataframe\n",
    "    return master"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
