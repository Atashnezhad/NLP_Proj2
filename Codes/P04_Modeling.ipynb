{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import eli5\n",
    "\n",
    "import regex as re\n",
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>over_18</th>\n",
       "      <th>post_hint</th>\n",
       "      <th>score</th>\n",
       "      <th>text_merged</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>illichian</td>\n",
       "      <td>1579413305</td>\n",
       "      <td>i.imgur.com</td>\n",
       "      <td>eqsltj</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>link</td>\n",
       "      <td>1</td>\n",
       "      <td>star shine saturn ring</td>\n",
       "      <td>NASA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>itstie</td>\n",
       "      <td>1579412680</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>eqsibf</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Empty</td>\n",
       "      <td>1</td>\n",
       "      <td>smithsonian nation air space museum</td>\n",
       "      <td>NASA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NASA_POTD_bot</td>\n",
       "      <td>1579410507</td>\n",
       "      <td>apod.nasa.gov</td>\n",
       "      <td>eqs6cb</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Empty</td>\n",
       "      <td>1</td>\n",
       "      <td>incred expand crab nebula</td>\n",
       "      <td>NASA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMC-Eagle85</td>\n",
       "      <td>1579410277</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>eqs4zd</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>Empty</td>\n",
       "      <td>1</td>\n",
       "      <td>columbia readi st</td>\n",
       "      <td>NASA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BorisTheSpacePerson</td>\n",
       "      <td>1579404939</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>eqr7wu</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Empty</td>\n",
       "      <td>1</td>\n",
       "      <td>went ksc christma got see made interest spacef...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author  created_utc         domain      id  num_comments  \\\n",
       "0            illichian   1579413305    i.imgur.com  eqsltj             2   \n",
       "1               itstie   1579412680      i.redd.it  eqsibf             0   \n",
       "2        NASA_POTD_bot   1579410507  apod.nasa.gov  eqs6cb             0   \n",
       "3          AMC-Eagle85   1579410277      i.redd.it  eqs4zd             6   \n",
       "4  BorisTheSpacePerson   1579404939      i.redd.it  eqr7wu             0   \n",
       "\n",
       "   over_18 post_hint  score  \\\n",
       "0    False      link      1   \n",
       "1    False     Empty      1   \n",
       "2    False     Empty      1   \n",
       "3    False     Empty      1   \n",
       "4    False     Empty      1   \n",
       "\n",
       "                                         text_merged subreddit  target  \n",
       "0                             star shine saturn ring      NASA       1  \n",
       "1                smithsonian nation air space museum      NASA       1  \n",
       "2                          incred expand crab nebula      NASA       1  \n",
       "3                                  columbia readi st      NASA       1  \n",
       "4  went ksc christma got see made interest spacef...      NASA       1  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_reddit = pickle.load(open('../DataSet/df_reddit_for_model.pkl', 'rb'))\n",
    "df_reddit = pickle.load(open('../DataSet/df_reddit.pkl', 'rb'))\n",
    "df_reddit['target'] = df_reddit['subreddit'].replace({\"NASA\": 1, \"Space_discussion\": 0})\n",
    "df_reddit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining X and Y variables and use train test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['text_merged']\n",
    "y = df_reddit['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['http', 'would', 'com']\n",
    "stopwords.extend(newStopWords)\n",
    "\n",
    "\n",
    "# tvec = TfidfVectorizer()\n",
    "cvec = CountVectorizer(stop_words=stopwords, min_df=4, max_df=1.0,\n",
    "                       ngram_range=(1,2),max_features = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_mat = cvec.fit_transform(df_reddit['text_merged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_df = pd.DataFrame(term_mat.toarray(), \n",
    "                       columns=cvec.get_feature_names())\n",
    "term_df.insert(0, 'targets', df_reddit['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targets</th>\n",
       "      <th>aa</th>\n",
       "      <th>ab</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>abl see</th>\n",
       "      <th>aboard</th>\n",
       "      <th>aboard intern</th>\n",
       "      <th>aboard space</th>\n",
       "      <th>abort</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>youlikebet</th>\n",
       "      <th>young</th>\n",
       "      <th>youtu</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero graviti</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zubrin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   targets  aa  ab  abil  abl  abl see  aboard  aboard intern  aboard space  \\\n",
       "0        1   0   0     0    0        0       0              0             0   \n",
       "1        1   0   0     0    0        0       0              0             0   \n",
       "2        1   0   0     0    0        0       0              0             0   \n",
       "3        1   0   0     0    0        0       0              0             0   \n",
       "4        1   0   0     0    0        0       0              0             0   \n",
       "\n",
       "   abort   ...    york  youlikebet  young  youtu  zealand  zero  zero graviti  \\\n",
       "0      0   ...       0           0      0      0        0     0             0   \n",
       "1      0   ...       0           0      0      0        0     0             0   \n",
       "2      0   ...       0           0      0      0        0     0             0   \n",
       "3      0   ...       0           0      0      0        0     0             0   \n",
       "4      0   ...       0           0      0      0        0     0             0   \n",
       "\n",
       "   zone  zoom  zubrin  \n",
       "0     0     0       0  \n",
       "1     0     0       0  \n",
       "2     0     0       0  \n",
       "3     0     0       0  \n",
       "4     0     0       0  \n",
       "\n",
       "[5 rows x 3001 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_nasa = list(term_df.groupby('targets').\n",
    "    mean().T.sort_values(1, ascending=False).head(20).index)\n",
    "\n",
    "top_words_space_dis = list(term_df.groupby('targets').\n",
    "    mean().T.sort_values(0, ascending=False).head(20).index)\n",
    "\n",
    "top_words_overlap = [word for word in top_words_space_dis if word in top_words_nasa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_words_nasa\n",
      "\n",
      " ['nasa', 'space', 'moon', 'apollo', 'earth', 'astronaut', 'mission', 'mar', 'year', 'launch', 'work', 'like', 'one', 'first', 'get', 'new', 'amp', 'time', 'go', 'station'] \n",
      "\n",
      "top_words_space_dis\n",
      "\n",
      " ['space', 'earth', 'nasa', 'moon', 'time', 'amp', 'star', 'year', 'planet', 'like', 'mar', 'launch', 'univers', 'could', 'new', 'hole', 'first', 'black', 'one', 'black hole'] \n",
      "\n",
      "top_words_overlap\n",
      "\n",
      " ['space', 'earth', 'nasa', 'moon', 'time', 'amp', 'year', 'like', 'mar', 'launch', 'new', 'first', 'one']\n"
     ]
    }
   ],
   "source": [
    "print('top_words_nasa\\n\\n', top_words_nasa,\n",
    "      '\\n\\ntop_words_space_dis\\n\\n', top_words_space_dis,\n",
    "      '\\n\\ntop_words_overlap\\n\\n', top_words_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.8376666666666667\n",
      "test score 0.7513333333333333\n"
     ]
    }
   ],
   "source": [
    "X_train_features = cvec.fit_transform(X_train)\n",
    "X_test_features = cvec.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(solver='sag', \n",
    "                        max_iter=3000)\n",
    "\n",
    "lr.fit(X_train_features, y_train);\n",
    "\n",
    "lr.score(X_train_features, y_train); \n",
    "print('train score', lr.score(X_train_features, y_train))\n",
    "\n",
    "lr.score(X_test_features, y_test); \n",
    "print('test score', lr.score(X_test_features, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7347777777777779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.12, 'max_depth': 4, 'n_estimators': 150}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gboost = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, \n",
    "                                    n_estimators=100, subsample=1.0, criterion='friedman_mse', \n",
    "                                    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                    max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                    init=None, random_state=None, max_features=None, verbose=0, \n",
    "                                    max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, \n",
    "                                    n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "gboost_params = {\n",
    "    'max_depth': [2,3,4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [.08, .1, .12]\n",
    "}\n",
    "gs = GridSearchCV(gboost, param_grid=gboost_params, cv=3)\n",
    "gs.fit(X_train_features, y_train)\n",
    "\n",
    "gs_M7 = GridSearchCV(gboost, param_grid=gboost_params, cv=3)\n",
    "gs_M7.fit(X_train_features, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7343333333333333\n"
     ]
    }
   ],
   "source": [
    "gboost = GradientBoostingClassifier(loss='deviance', learning_rate=0.12, \n",
    "                                    n_estimators=150, subsample=1.0, criterion='friedman_mse', \n",
    "                                    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                    max_depth=4, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                    init=None, random_state=None, max_features=None, verbose=0, \n",
    "                                    max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, \n",
    "                                    n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "gboost.fit(X_train_features, y_train)\n",
    "print(gboost.score(X_test_features, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=50, activation='relu', solver='adam', \n",
    "                    alpha=0.0001, batch_size='auto', learning_rate='constant', \n",
    "                    learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, \n",
    "                    random_state=None, tol=0.0001, verbose=False, warm_start=False, \n",
    "                    momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n",
    "                    validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, \n",
    "                    n_iter_no_change=10, max_fun=15000)\n",
    "\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [20,50,100],\n",
    "    'activation': ['identity', 'tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam']\n",
    "}\n",
    "mlpg = GridSearchCV(mlp, param_grid=mlp_params, cv=3)\n",
    "mlpg.fit(X_train_features, y_train)\n",
    "\n",
    "\n",
    "print(mlpg.best_score_)\n",
    "mlpg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7033333333333334"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train_features,y_train)\n",
    "mlp.score(X_train_features, y_train)\n",
    "\n",
    "mlp.score(X_test_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.6950585175552666, 0.7126666666666667)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = mlp.predict(X_test_features)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "# View confusion matrix\n",
    "\n",
    "#plot_confusion_matrix(gs, X_test_features, y_test, cmap='Blues', values_format='d');\n",
    "\n",
    "Accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "Precision = tp / (tp + fp)\n",
    "Recall = tp / (tp + fn)\n",
    "\n",
    "Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
