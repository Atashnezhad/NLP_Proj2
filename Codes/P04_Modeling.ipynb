{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import eli5\n",
    "\n",
    "import regex as re\n",
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ent</th>\n",
       "      <th>text_merged</th>\n",
       "      <th>word_count*</th>\n",
       "      <th>sentiment_score*</th>\n",
       "      <th>target</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>polarity</th>\n",
       "      <th>polarity_VSA</th>\n",
       "      <th>lower_sent</th>\n",
       "      <th>tagged_sent</th>\n",
       "      <th>text_complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>star shine saturn ring</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>star shine saturn ring</td>\n",
       "      <td>[(star, NN), (shine, NN), (saturn, NN), (ring,...</td>\n",
       "      <td>92.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>smithsonian nation air space museum</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>smithsonian nation air space museum</td>\n",
       "      <td>[(smithsonian, JJ), (nation, NN), (air, NN), (...</td>\n",
       "      <td>15.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ent                          text_merged  word_count*  sentiment_score*  \\\n",
       "0    1               star shine saturn ring            4               0.0   \n",
       "1    2  smithsonian nation air space museum            5               0.0   \n",
       "\n",
       "   target  polarity_score  polarity  polarity_VSA  \\\n",
       "0       1             0.0         1             1   \n",
       "1       1             0.0         1             1   \n",
       "\n",
       "                            lower_sent  \\\n",
       "0               star shine saturn ring   \n",
       "1  smithsonian nation air space museum   \n",
       "\n",
       "                                         tagged_sent  text_complexity  \n",
       "0  [(star, NN), (shine, NN), (saturn, NN), (ring,...            92.80  \n",
       "1  [(smithsonian, JJ), (nation, NN), (air, NN), (...            15.64  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_reddit = pickle.load(open('../DataSet/df_reddit_for_model.pkl', 'rb'))\n",
    "df_reddit = pickle.load(open('../DataSet/df_reddit_for_model.pkl', 'rb'))\n",
    "EDA_df = pickle.load(open('../DataSet/EDA_all_for_model.pkl', 'rb'))\n",
    "# df_reddit['target'] = df_reddit['subreddit'].replace({\"NASA\": 1, \"Space_discussion\": 0})\n",
    "df_reddit.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>''</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>DT</th>\n",
       "      <th>EX</th>\n",
       "      <th>FW</th>\n",
       "      <th>IN</th>\n",
       "      <th>JJ</th>\n",
       "      <th>JJR</th>\n",
       "      <th>...</th>\n",
       "      <th>WP</th>\n",
       "      <th>WP$</th>\n",
       "      <th>WRB</th>\n",
       "      <th>ent</th>\n",
       "      <th>word_count*</th>\n",
       "      <th>sentiment_score*</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>polarity</th>\n",
       "      <th>polarity_VSA</th>\n",
       "      <th>text_complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target   ''   CC   CD   DT   EX   FW   IN   JJ  JJR  ...   WP  WP$  WRB  \\\n",
       "0       1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1       1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "   ent  word_count*  sentiment_score*  polarity_score  polarity  polarity_VSA  \\\n",
       "0    1            4               0.0             0.0         1             1   \n",
       "1    2            5               0.0             0.0         1             1   \n",
       "\n",
       "   text_complexity  \n",
       "0            92.80  \n",
       "1            15.64  \n",
       "\n",
       "[2 rows x 43 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDA_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining X and Y variables and use train test split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reddit['text_merged']\n",
    "y = df_reddit['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(min_df=4, max_df=1.0,\n",
    "                       ngram_range=(1,2),max_features = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_mat = cvec.fit_transform(df_reddit['text_merged'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_df = pd.DataFrame(term_mat.toarray(), \n",
    "                       columns=cvec.get_feature_names())\n",
    "term_df.insert(0, 'targets', df_reddit['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>targets</th>\n",
       "      <th>aa</th>\n",
       "      <th>ab</th>\n",
       "      <th>abil</th>\n",
       "      <th>abl</th>\n",
       "      <th>abl get</th>\n",
       "      <th>abl see</th>\n",
       "      <th>aboard</th>\n",
       "      <th>aboard intern</th>\n",
       "      <th>aboard space</th>\n",
       "      <th>...</th>\n",
       "      <th>york</th>\n",
       "      <th>youlikebet</th>\n",
       "      <th>young</th>\n",
       "      <th>youtu</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero graviti</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zubrin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   targets  aa  ab  abil  abl  abl get  abl see  aboard  aboard intern  \\\n",
       "0        1   0   0     0    0        0        0       0              0   \n",
       "1        1   0   0     0    0        0        0       0              0   \n",
       "2        1   0   0     0    0        0        0       0              0   \n",
       "3        1   0   0     0    0        0        0       0              0   \n",
       "4        1   0   0     0    0        0        0       0              0   \n",
       "\n",
       "   aboard space  ...  york  youlikebet  young  youtu  zealand  zero  \\\n",
       "0             0  ...     0           0      0      0        0     0   \n",
       "1             0  ...     0           0      0      0        0     0   \n",
       "2             0  ...     0           0      0      0        0     0   \n",
       "3             0  ...     0           0      0      0        0     0   \n",
       "4             0  ...     0           0      0      0        0     0   \n",
       "\n",
       "   zero graviti  zone  zoom  zubrin  \n",
       "0             0     0     0       0  \n",
       "1             0     0     0       0  \n",
       "2             0     0     0       0  \n",
       "3             0     0     0       0  \n",
       "4             0     0     0       0  \n",
       "\n",
       "[5 rows x 4001 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets try Logistic Regression algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pactools\n",
    "# !/Users/amin/anaconda3/bin/python -m pip install pactools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A comperhensive grid search on lr model Done.\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('logreg', LogisticRegression())\n",
    "])\n",
    "\n",
    "\n",
    "pipe_params = {\n",
    "    'cvec__max_features': [2_000, 4_000],\n",
    "    'cvec__ngram_range': [(1,1), (1,2)],\n",
    "    'logreg__penalty': ['l2', 'none'],\n",
    "    'logreg__solver': ['newton-cg', 'sag'],\n",
    "    'logreg__max_iter': [100, 300]\n",
    "}\n",
    "\n",
    "lr = GridSearchCV(pipe, # what object are we optimizing?\n",
    "                  param_grid=pipe_params, # what parameters values are we searching?\n",
    "                  cv=5, \n",
    "                  scoring='accuracy', \n",
    "                  verbose=2) \n",
    "\n",
    "lr.fit(X_train, y_train);\n",
    "print('lr.best_params_\\n', lr.best_params_)\n",
    "\n",
    "\n",
    "\n",
    "### Here is the best results of above grid search.\n",
    "\n",
    "```python\n",
    "\n",
    "print('lr.best_params_\\n', lr.best_params_)\n",
    "\n",
    "lr.best_params_\n",
    " {'cvec__max_features': 4000, 'cvec__ngram_range': (1, 2), \n",
    " 'logreg__max_iter': 300, 'logreg__penalty': 'none', 'logreg__solver': 'sag'}\n",
    " \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going with best lr hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.7956666666666666\n",
      "test score 0.7443333333333333\n"
     ]
    }
   ],
   "source": [
    "cvec = CountVectorizer(max_features= 4000,\n",
    "                       min_df=4, max_df=1.0,\n",
    "                       ngram_range=(1,2))\n",
    "X_train_features = cvec.fit_transform(X_train)\n",
    "X_test_features = cvec.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression(penalty='none', dual=False, tol=0.0001, \n",
    "                         C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "                         class_weight=None, random_state=None, solver='sag', \n",
    "                         max_iter=300, multi_class='auto', verbose=0, \n",
    "                         warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "lr.fit(X_train_features, y_train);\n",
    "\n",
    "lr.score(X_train_features, y_train); \n",
    "print('train score', lr.score(X_train_features, y_train))\n",
    "\n",
    "lr.score(X_test_features, y_test); \n",
    "print('test score', lr.score(X_test_features, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the Accuracy, Precision, Recall parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7443333333333333, 0.768892149669846, 0.6986666666666667)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = lr.predict(X_test_features)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "# View confusion matrix\n",
    "\n",
    "# plot_confusion_matrix(gs, X_test_features, y_test, cmap='Blues', values_format='d');\n",
    "Accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "Precision = tp / (tp + fp)\n",
    "Recall = tp / (tp + fn)\n",
    "\n",
    "Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying gradient boosting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using All CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:    7.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7342222222222222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.12, 'max_depth': 4, 'n_estimators': 150}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gboost = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, \n",
    "                                    n_estimators=100, subsample=1.0, criterion='friedman_mse', \n",
    "                                    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                    max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                    init=None, random_state=None, max_features=None, verbose=0, \n",
    "                                    max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, \n",
    "                                    n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "gboost_params = {\n",
    "    'max_depth': [2,3,4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [.08, .1, .12]\n",
    "}\n",
    "gs = GridSearchCV(gboost, param_grid=gboost_params, \n",
    "                  cv=3, verbose=1, n_jobs=-1)\n",
    "gs.fit(X_train_features, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just took ~ 8 seconds.\n",
    "Now using 1 CPU.\n",
    "### Using One CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:  1.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7330000000000001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.12, 'max_depth': 4, 'n_estimators': 150}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gboost = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, \n",
    "                                    n_estimators=100, subsample=1.0, criterion='friedman_mse', \n",
    "                                    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                    max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                    init=None, random_state=None, max_features=None, verbose=0, \n",
    "                                    max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, \n",
    "                                    n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "gboost_params = {\n",
    "    'max_depth': [2,3,4],\n",
    "    'n_estimators': [100, 125, 150],\n",
    "    'learning_rate': [.08, .1, .12]\n",
    "}\n",
    "gs = GridSearchCV(gboost, param_grid=gboost_params, \n",
    "                  cv=3, verbose=1, n_jobs=None)\n",
    "gs.fit(X_train_features, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "60 seconds compare to 8 seconds! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going with the best hyperparameters for gboost. Notice grid search convergence to max values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7323333333333333\n"
     ]
    }
   ],
   "source": [
    "gboost = GradientBoostingClassifier(loss='deviance', learning_rate=0.12, \n",
    "                                    n_estimators=150, subsample=1.0, criterion='friedman_mse', \n",
    "                                    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                    max_depth=4, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                    init=None, random_state=None, max_features=None, verbose=0, \n",
    "                                    max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, \n",
    "                                    n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "gboost.fit(X_train_features, y_train)\n",
    "print(gboost.score(X_test_features, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the Accuracy, Precision, Recall parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7323333333333333, 0.8065083553210203, 0.6113333333333333)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = gboost.predict(X_test_features)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "# View confusion matrix\n",
    "\n",
    "#plot_confusion_matrix(gs, X_test_features, y_test, cmap='Blues', values_format='d');\n",
    "Accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "Precision = tp / (tp + fp)\n",
    "Recall = tp / (tp + fn)\n",
    "\n",
    "Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ANN model as follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using All CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  4.6min finished\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'mlpg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-f85647a69ed8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmlpg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mmlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mlpg' is not defined"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=50, activation='relu', solver='adam', \n",
    "                    alpha=0.0001, batch_size='auto', learning_rate='constant', \n",
    "                    learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, \n",
    "                    random_state=None, tol=0.0001, verbose=False, warm_start=False, \n",
    "                    momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n",
    "                    validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, \n",
    "                    n_iter_no_change=10, max_fun=15000)\n",
    "\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [20,50],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam']\n",
    "}\n",
    "mlp = GridSearchCV(mlp, param_grid=mlp_params, cv=3, verbose=1, n_jobs=-1)\n",
    "mlp.fit(X_train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7447777777777778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh', 'hidden_layer_sizes': 20, 'solver': 'sgd'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mlp.best_score_)\n",
    "mlp.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going with the best hyperparameters for ANN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7556666666666667, 0.7672473867595819, 0.734)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = mlp.predict(X_test_features)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "# View confusion matrix\n",
    "\n",
    "#plot_confusion_matrix(gs, X_test_features, y_test, cmap='Blues', values_format='d');\n",
    "Accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "Precision = tp / (tp + fp)\n",
    "Recall = tp / (tp + fn)\n",
    "\n",
    "Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking out the Accuracy, Precision, Recall parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7406666666666667\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=50, activation='relu', solver='sgd', \n",
    "                    alpha=0.0001, batch_size='auto', learning_rate='constant', \n",
    "                    learning_rate_init=0.001, power_t=0.5, max_iter=100, shuffle=True, \n",
    "                    random_state=None, tol=0.0001, verbose=False, warm_start=False, \n",
    "                    momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n",
    "                    validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, \n",
    "                    n_iter_no_change=10, max_fun=15000)\n",
    "\n",
    "\n",
    "mlp.fit(X_train_features, y_train)\n",
    "print(mlp.score(X_test_features, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets use EDA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>''</th>\n",
       "      <th>CC</th>\n",
       "      <th>CD</th>\n",
       "      <th>DT</th>\n",
       "      <th>EX</th>\n",
       "      <th>FW</th>\n",
       "      <th>IN</th>\n",
       "      <th>JJ</th>\n",
       "      <th>JJR</th>\n",
       "      <th>...</th>\n",
       "      <th>WP</th>\n",
       "      <th>WP$</th>\n",
       "      <th>WRB</th>\n",
       "      <th>ent</th>\n",
       "      <th>word_count*</th>\n",
       "      <th>sentiment_score*</th>\n",
       "      <th>polarity_score</th>\n",
       "      <th>polarity</th>\n",
       "      <th>polarity_VSA</th>\n",
       "      <th>text_complexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   target   ''   CC   CD   DT   EX   FW   IN   JJ  JJR  ...   WP  WP$  WRB  \\\n",
       "0       1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1       1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "   ent  word_count*  sentiment_score*  polarity_score  polarity  polarity_VSA  \\\n",
       "0    1            4               0.0             0.0         1             1   \n",
       "1    2            5               0.0             0.0         1             1   \n",
       "\n",
       "   text_complexity  \n",
       "0            92.80  \n",
       "1            15.64  \n",
       "\n",
       "[2 rows x 43 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EDA_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data frame as numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = EDA_df['target']\n",
    "X = EDA_df.drop(columns = ['target'])\n",
    "X_arr = np.array(X)\n",
    "y_arr = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_arr,\n",
    "                                                    y_arr,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state = 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A comperhensive grid search on lr model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr_model = LogisticRegression(penalty='none', dual=False, tol=0.0001, \n",
    "#                               C=1.0, fit_intercept=True, intercept_scaling=1, \n",
    "#                               class_weight=None, random_state=None, solver='sag', \n",
    "#                               max_iter=300, multi_class='auto', verbose=0, \n",
    "#                               warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "\n",
    "# pipe_params = {'lr_model__penalty': ['l2', 'none'], \n",
    "#                'lr_model__solver': ['newton-cg', 'sag'], \n",
    "#                'lr_model__max_iter': [100, 300]}\n",
    "\n",
    "\n",
    "# lr = GridSearchCV(lr_model, param_grid=pipe_params, \n",
    "#                   cv=3, verbose=1, n_jobs=-1)\n",
    "\n",
    "# lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score 0.825\n",
      "test score 0.8276666666666667\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='sag', \n",
    "                        max_iter=5000)\n",
    "\n",
    "lr.fit(X_train, y_train);\n",
    "\n",
    "lr.score(X_train, y_train); \n",
    "print('train score', lr.score(X_train, y_train))\n",
    "\n",
    "lr.score(X_test, y_test); \n",
    "print('test score', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8276666666666667, 0.9685414680648237, 0.6773333333333333)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = lr.predict(X_test)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "# View confusion matrix\n",
    "\n",
    "#plot_confusion_matrix(gs, X_test_features, y_test, cmap='Blues', values_format='d');\n",
    "Accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "Precision = tp / (tp + fp)\n",
    "Recall = tp / (tp + fn)\n",
    "\n",
    "Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:   21.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8345555555555556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.08, 'max_depth': 2, 'n_estimators': 100}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gboost = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, \n",
    "                                    n_estimators=100, subsample=1.0, criterion='friedman_mse', \n",
    "                                    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                    max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                    init=None, random_state=None, max_features=None, verbose=0, \n",
    "                                    max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, \n",
    "                                    n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "gboost_params = {\n",
    "    'max_depth': [2,3,4],\n",
    "    'n_estimators': [100, 250, 500],\n",
    "    'learning_rate': [.08, .1, .12]\n",
    "}\n",
    "gs = GridSearchCV(gboost, param_grid=gboost_params, \n",
    "                  cv=3, verbose=1, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and going down with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8353333333333334\n"
     ]
    }
   ],
   "source": [
    "gboost = GradientBoostingClassifier(loss='deviance', learning_rate=0.08, \n",
    "                                    n_estimators=100, subsample=1.0, criterion='friedman_mse', \n",
    "                                    min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                                    max_depth=2, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                                    init=None, random_state=None, max_features=None, verbose=0, \n",
    "                                    max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, \n",
    "                                    n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
    "gboost.fit(X_train, y_train)\n",
    "print(gboost.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8353333333333334, 1.0, 0.6706666666666666)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = gboost.predict(X_test)\n",
    "\n",
    "# Save confusion matrix values\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "# View confusion matrix\n",
    "\n",
    "#plot_confusion_matrix(gs, X_test_features, y_test, cmap='Blues', values_format='d');\n",
    "Accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "Precision = tp / (tp + fp)\n",
    "Recall = tp / (tp + fn)\n",
    "\n",
    "Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN using all CPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    9.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=MLPClassifier(hidden_layer_sizes=50), n_jobs=-1,\n",
       "             param_grid={'activation': ['tanh', 'relu'],\n",
       "                         'hidden_layer_sizes': [20, 50],\n",
       "                         'solver': ['lbfgs', 'sgd', 'adam']},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=50, activation='relu', solver='adam', \n",
    "                    alpha=0.0001, batch_size='auto', learning_rate='constant', \n",
    "                    learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, \n",
    "                    random_state=None, tol=0.0001, verbose=False, warm_start=False, \n",
    "                    momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n",
    "                    validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, \n",
    "                    n_iter_no_change=10, max_fun=15000)\n",
    "\n",
    "\n",
    "mlp_params = {\n",
    "    'hidden_layer_sizes': [20,50],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam']\n",
    "}\n",
    "mlp = GridSearchCV(mlp, param_grid=mlp_params, cv=3, verbose=1, n_jobs=-1)\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8273333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu', 'hidden_layer_sizes': 50, 'solver': 'adam'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mlp.best_score_)\n",
    "mlp.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8323333333333334\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=50, activation='relu', solver='adam', \n",
    "                    alpha=0.0001, batch_size='auto', learning_rate='constant', \n",
    "                    learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, \n",
    "                    random_state=None, tol=0.0001, verbose=False, warm_start=False, \n",
    "                    momentum=0.9, nesterovs_momentum=True, early_stopping=False, \n",
    "                    validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, \n",
    "                    n_iter_no_change=10, max_fun=15000)\n",
    "\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "print(mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8323333333333334, 0.9911330049261083, 0.6706666666666666)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = mlp.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "# View confusion matrix\n",
    "#plot_confusion_matrix(gs, X_test_features, y_test, cmap='Blues', values_format='d');\n",
    "Accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "Precision = tp / (tp + fp)\n",
    "Recall = tp / (tp + fn)\n",
    "\n",
    "Accuracy, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
